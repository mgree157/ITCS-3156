{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ece07fc",
   "metadata": {},
   "source": [
    "\n",
    "# Homework: Logistic Regression\n",
    "\n",
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    " \\newcommand{\\wv}{\\mathbf{w}}\n",
    " \\newcommand{\\yv}{\\mathbf{y}}\n",
    " \\newcommand{\\zv}{\\mathbf{z}}\n",
    " \\newcommand{\\uv}{\\mathbf{u}}\n",
    " \\newcommand{\\vv}{\\mathbf{v}}\n",
    " \\newcommand{\\tv}{\\mathbf{t}}\n",
    " \\newcommand{\\Chi}{\\mathcal{X}}\n",
    " \\newcommand{\\R}{\\rm I\\!R}\n",
    " \\newcommand{\\sign}{\\text{sign}}\n",
    " \\newcommand{\\Tm}{\\mathbf{T}}\n",
    " \\newcommand{\\Xm}{\\mathbf{X}}\n",
    " \\newcommand{\\Zm}{\\mathbf{Z}}\n",
    " \\newcommand{\\Im}{\\mathbf{I}}\n",
    " \\newcommand{\\Um}{\\mathbf{U}}\n",
    " \\newcommand{\\Vm}{\\mathbf{V}} \n",
    " \\newcommand{\\muv}{\\boldsymbol\\mu}\n",
    " \\newcommand{\\Sigmav}{\\boldsymbol\\Sigma}\n",
    " \\newcommand{\\Lambdav}{\\boldsymbol\\Lambda}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb6745d",
   "metadata": {},
   "source": [
    "## Name: <span style=\"color:blue\"> Myles Green </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d2f29",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbac6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Callable\n",
    "import os\n",
    "import gc\n",
    "import traceback\n",
    "import warnings\n",
    "from pdb import set_trace\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaae1f5d-ce69-438b-9359-110cf0074f1b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6661fa165181d5e9c310755a7a5a6f7e",
     "grade": true,
     "grade_id": "cell-0a227fc64c6ed0ba",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TodoCheckFailed(Exception):\n",
    "    pass\n",
    "\n",
    "def todo_check(asserts, mute=False, success_msg=\"\", **kwargs):\n",
    "    locals().update(kwargs)\n",
    "    failed_err = \"You passed {}/{} and FAILED the following code checks:\\n{}\"\n",
    "    failed = \"\"\n",
    "    n_failed = 0\n",
    "    for check, (condi, err) in enumerate(asserts):\n",
    "        exc_failed = False\n",
    "        if isinstance(condi, str):\n",
    "            try:\n",
    "                passed = eval(condi)\n",
    "            except Exception:\n",
    "                exc_failed = True\n",
    "                n_failed += 1\n",
    "                failed += f\"\\nCheck [{check+1}]: Failed to execute check [{check+1}] due to the following error...\\n{traceback.format_exc()}\"\n",
    "        elif isinstance(condi, bool):\n",
    "            passed = condi\n",
    "        else:\n",
    "            raise ValueError(\"asserts must be a list of strings or bools\")\n",
    "\n",
    "        if not exc_failed and not passed:\n",
    "            n_failed += 1\n",
    "            failed += f\"\\nCheck [{check+1}]: Failed\\n\\tTip: {err}\\n\"\n",
    "\n",
    "    if len(failed) != 0:\n",
    "        passed = len(asserts) - n_failed\n",
    "        err = failed_err.format(passed, len(asserts), failed)\n",
    "        raise TodoCheckFailed(err.format(failed))\n",
    "    if not mute: print(f\"Your code PASSED all the code checks! {success_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27572d05-2cbd-47a7-82b6-164d1069a800",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "In this assignment, you will be implementing logistic regression. You will be working with the classical multi-classification MNIST dataset. \n",
    "\n",
    "Your job is to read through the assignment and fill in any code segments that are marked by `TODO` headers and comments. Some TODOs will have a `todo_check()` function which will give you a rough estimate of whether your code is functioning as excepted. Other's might not have these checks, like visualization TODOs. Regardless,  all the correct outputs are given below each code cell. It might be useful to copy the contents of certain TODO cells into a new cell so you can try to match the desired output with the output produced by your own code! For visualization TODOs, you simply have to have a plot that looks similar. You can change aspects such as color, titles, or x/y-axis labels if you so wish.\n",
    "\n",
    "At any point, if you feel lost concerning how to program a specific TODO, take some time and visit the official documentation for each library and read about the methods/functions that you need to use.\n",
    "\n",
    "## Submission\n",
    "\n",
    "1. Save the notebook.\n",
    "2. Enter your name in the appropriate markdown cell provided at the top of the notebook.\n",
    "3. Select `Kernel` -> `Restart Kernel and Run All Cells`. This will restart the kernel and run all cells. Make sure everything runs without errors and double-check the outputs are as you desire!\n",
    "4. Submit the `.ipynb` notebook on Canvas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a4206",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Sign Language MNIST dataset \n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png\" width=700 height=500></center>\n",
    "\n",
    "This assignment will have you tackle the famous MNIST classification problem, where the goal is to classify 9 different handwritten digits (0- 9). The MNIST dataset is a frequently used dataset when first being introduced to classification in machine learning. [UCI](https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits) description of the dataset is as follows:\n",
    "\n",
    "    :Number of Instances: 1797\n",
    "    :Number of Attributes: 64\n",
    "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
    "    :Missing Attribute Values: None\n",
    "    \n",
    "    The data set contains images of hand-written digits: 10 classes where\n",
    "    each class refers to a digit.\n",
    "    \n",
    "    Preprocessing programs made available by NIST were used to extract\n",
    "    normalized bitmaps of handwritten digits from a preprinted form. From a\n",
    "    total of 43 people, 30 contributed to the training set and different 13\n",
    "    to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
    "    4x4 and the number of on pixels are counted in each block. This generates\n",
    "    an input matrix of 8x8 where each element is an integer in the range\n",
    "    0..16. This reduces dimensionality and gives invariance to small\n",
    "    distortions.\n",
    "\n",
    "Take note of the following important aspects about the data:\n",
    "\n",
    "- The data features correspond to 64 pixels which means the images are 8x8. Often, it is best practice to flatten the 2D images into a single 1D array such that the width multiplied by the height determines the number of features (i.e., pixels).\n",
    "- Pixel values are gray scale and range from 0-255.\n",
    "- There are 9 classes, one for each digit between 0 and 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386732fe",
   "metadata": {},
   "source": [
    "#### TODO 1 (5 points): Data Loading\n",
    "Complete the TODO by loading the Iris dataset using Sklearn.\n",
    "\n",
    "1. Load the MNIST dataset by calling Sklearn's `load_digits()` function and pass the parameter the returns the data as a pandas DataFrame. Store the output into `mnist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb647af-cc6d-4b55-ab74-29199734d48c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "268747bb490cec04943960fc4f79eb2b",
     "grade": false,
     "grade_id": "cell-e8336b0af38b5a8f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 1\n",
    "from sklearn.datasets import load_digits\n",
    "mnist = None\n",
    "mnist = load_digits(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba5e514-033f-4d5e-b3a5-ae2afdc3b73e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f536e96401b44dfb01dc03347412c313",
     "grade": true,
     "grade_id": "cell-826a687aa153f969",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your code PASSED all the code checks! \n"
     ]
    }
   ],
   "source": [
    "todo_check([\n",
    "    (\"isinstance(mnist, sklearn.utils.Bunch)\", \"'mnist' is not of type Bunch\"),\n",
    "    (\"isinstance(mnist.data, pd.DataFrame)\", \"mnist.data is not a Pandas DataFrame. Make sure to pass the correct argument to load_digits().\"),\n",
    "    (\"isinstance(mnist.target, pd.Series)\", \"mnist.target is not a Pandas series. Make sure to pass the correct argument to load_digits().\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b404fb07-bbb6-45f6-b230-966aecb102c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_6_6</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
       "0           0.0        0.0        5.0       13.0        9.0        1.0   \n",
       "1           0.0        0.0        0.0       12.0       13.0        5.0   \n",
       "2           0.0        0.0        0.0        4.0       15.0       12.0   \n",
       "3           0.0        0.0        7.0       15.0       13.0        1.0   \n",
       "4           0.0        0.0        0.0        1.0       11.0        0.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1792        0.0        0.0        4.0       10.0       13.0        6.0   \n",
       "1793        0.0        0.0        6.0       16.0       13.0       11.0   \n",
       "1794        0.0        0.0        1.0       11.0       15.0        1.0   \n",
       "1795        0.0        0.0        2.0       10.0        7.0        0.0   \n",
       "1796        0.0        0.0       10.0       14.0        8.0        1.0   \n",
       "\n",
       "      pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_6  pixel_6_7  \\\n",
       "0           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "2           0.0        0.0        0.0        0.0  ...        5.0        0.0   \n",
       "3           0.0        0.0        0.0        8.0  ...        9.0        0.0   \n",
       "4           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "1792        0.0        0.0        0.0        1.0  ...        4.0        0.0   \n",
       "1793        1.0        0.0        0.0        0.0  ...        1.0        0.0   \n",
       "1794        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1795        0.0        0.0        0.0        0.0  ...        2.0        0.0   \n",
       "1796        0.0        0.0        0.0        2.0  ...        8.0        0.0   \n",
       "\n",
       "      pixel_7_0  pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  \\\n",
       "0           0.0        0.0        6.0       13.0       10.0        0.0   \n",
       "1           0.0        0.0        0.0       11.0       16.0       10.0   \n",
       "2           0.0        0.0        0.0        3.0       11.0       16.0   \n",
       "3           0.0        0.0        7.0       13.0       13.0        9.0   \n",
       "4           0.0        0.0        0.0        2.0       16.0        4.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1792        0.0        0.0        2.0       14.0       15.0        9.0   \n",
       "1793        0.0        0.0        6.0       16.0       14.0        6.0   \n",
       "1794        0.0        0.0        2.0        9.0       13.0        6.0   \n",
       "1795        0.0        0.0        5.0       12.0       16.0       12.0   \n",
       "1796        0.0        1.0        8.0       12.0       14.0       12.0   \n",
       "\n",
       "      pixel_7_6  pixel_7_7  \n",
       "0           0.0        0.0  \n",
       "1           0.0        0.0  \n",
       "2           9.0        0.0  \n",
       "3           0.0        0.0  \n",
       "4           0.0        0.0  \n",
       "...         ...        ...  \n",
       "1792        0.0        0.0  \n",
       "1793        0.0        0.0  \n",
       "1794        0.0        0.0  \n",
       "1795        0.0        0.0  \n",
       "1796        1.0        0.0  \n",
       "\n",
       "[1797 rows x 64 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2190dcfd-b4a1-47ca-b207-05f76c816a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       2\n",
       "3       3\n",
       "4       4\n",
       "       ..\n",
       "1792    9\n",
       "1793    0\n",
       "1794    8\n",
       "1795    9\n",
       "1796    8\n",
       "Name: target, Length: 1797, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17df0ad-71d6-4f32-837f-dcedcdc750a9",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb463e1-60e6-4d6e-b816-573cb1476324",
   "metadata": {},
   "source": [
    "#### TODO 2 (10 points): Sample Visualization\n",
    "Visualize, at least, one sample from each class in the MNIST dataset (total of 9 images should be displayed). To receive full points, the following requirements must be satisfied:\n",
    "\n",
    "- At least one image is displayed for each class (digits 0-9).\n",
    "- Each image is clearly labeled with the class label that it corresponds to.\n",
    "\n",
    "**Hints**\n",
    "- You will need to reshape the features into a 8x8 image.\n",
    "- You can use Matplotlib's `imshow()` function to plot images and `plt.cm.gray` as the color map to plot the image in gray-scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b66711-5c3a-4ca0-8e8b-2b33074b0c51",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8bbb925e424a94aef2faa4cea2df09f",
     "grade": true,
     "grade_id": "cell-d3d23fc6f2daece5",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO 2\n",
    "images = mnist.images  # Shape is (1797, 8, 8)\n",
    "labels = mnist.target  # Shape is (1797,)\n",
    "\n",
    "# Find one sample for each class (digits 0-9)\n",
    "unique_labels = np.unique(labels)\n",
    "samples = [images[np.where(labels == label)[0][0]] for label in unique_labels]\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, image in enumerate(samples):\n",
    "    plt.subplot(2, 5, i + 1)  \n",
    "    plt.imshow(image, cmap=plt.cm.gray)\n",
    "    plt.title(f\"Label: {i+1}\")  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1821db21",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7124b05-7591-4fef-9021-69bffe84938c",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ee570f-f478-43c6-bb10-5f6a732ce272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def get_train_valid_test_data(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "):\n",
    "    \"\"\" Randomizes and then splits the data into train, validation, and test sets.\n",
    "\n",
    "        Args:\n",
    "            X: Data given as a 2D matrix\n",
    "\n",
    "            y: Labels given as a vector \n",
    "    \"\"\"\n",
    "    X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, train_size=.8, random_state=42)\n",
    "    X_trn, X_vld, y_trn, y_vld = train_test_split(X_trn, y_trn, train_size=.8, random_state=42)\n",
    "\n",
    "    return X_trn, y_trn, X_vld, y_vld, X_tst, y_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3526f",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "#### TODO 3 (10 points): Preprocess Data\n",
    "\n",
    "Complete the `get_preprocessed_data()` function for performing data preprocessing by satisfying the below requirements. The function should return a set of NumPy arrays: `X_trn`, `y_trn`, `X_vld`, `y_vld`, `X_tst`, and `y_tst`. \n",
    "\n",
    "1. Convert the labels into a one-hot encoding using Sklearn's `OneHotEncoder` class or NumPy.\n",
    "2. Split the data and labels into train, validation and test sets.\n",
    "3. Standardize the training, validation, and testing data.\n",
    "4. Add the bias to the first column of the training, validation, and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c2ec06-eb62-470f-877d-50e1ec5465a9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52f266a5adc4ab7338309d25c111284a",
     "grade": false,
     "grade_id": "cell-f1e6f335b91ac2e6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_data() -> Tuple[np.ndarray]:\n",
    "    \"\"\" Gets preprocessed data for training, validation, and testing\n",
    "\n",
    "        Return:\n",
    "            A tuple of NumPy arrays where indices 0-1 \n",
    "            contain the training data/targets, indices 2-3\n",
    "            contain the validation data/targets, and 4-5\n",
    "            contain the testing data/targets.\n",
    "    \"\"\"\n",
    "    X_trn, y_trn, X_vld, y_vld, X_tst, y_tst= None, None, None, None, None, None\n",
    "    # TODO 3.1 - 3.4from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "    \n",
    "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "    \n",
    "    df = mnist.frame\n",
    "    X = df.drop(columns=[\"target\"]).to_numpy()\n",
    "    y = df[\"target\"].to_numpy()\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    encoded_y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "    \n",
    "    X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = get_train_valid_test_data(X, encoded_y)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_trn = scaler.fit_transform(X_trn)\n",
    "    X_vld = scaler.transform(X_vld)\n",
    "    X_tst = scaler.transform(X_tst)\n",
    "    \n",
    "    X_trn = np.hstack([np.ones((X_trn.shape[0], 1)), X_trn])\n",
    "    X_vld = np.hstack([np.ones((X_vld.shape[0], 1)), X_vld])\n",
    "    X_tst = np.hstack([np.ones((X_tst.shape[0], 1)), X_tst])\n",
    "    \n",
    "    return X_trn, y_trn, X_vld, y_vld, X_tst, y_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76ceab02-0f22-4940-8999-dbc294cf7497",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5540fba2f6f6834e4f86de8fd59f7b87",
     "grade": true,
     "grade_id": "cell-d696fe491e9f4104",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn shape: (1149, 65)\n",
      "y_trn shape: (1149, 10)\n",
      "X_vld shape: (288, 65)\n",
      "y_vld shape: (288, 10)\n",
      "X_tst shape: (360, 65)\n",
      "y_tst shape: (360, 10)\n",
      "Your code PASSED all the code checks! \n"
     ]
    }
   ],
   "source": [
    "def TEST_get_preprocessed_data():\n",
    "    X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = get_preprocessed_data()\n",
    "\n",
    "    print(f\"X_trn shape: {X_trn.shape}\")\n",
    "    print(f\"y_trn shape: {y_trn.shape}\")\n",
    "\n",
    "    print(f\"X_vld shape: {X_vld.shape}\")\n",
    "    print(f\"y_vld shape: {y_vld.shape}\")\n",
    "    \n",
    "    print(f\"X_tst shape: {X_tst.shape}\")\n",
    "    print(f\"y_tst shape: {y_tst.shape}\")\n",
    "    \n",
    "    todo_check([\n",
    "        # Split and one-hot tests\n",
    "        (\"isinstance(X_trn, np.ndarray)\", \"X_trn must be a NumPy array.\"),\n",
    "        (\"isinstance(X_vld, np.ndarray)\", \"X_vld must be a NumPy array.\"),\n",
    "        (\"isinstance(X_tst, np.ndarray)\", \"X_tst must be a NumPy array.\"),\n",
    "        (\"isinstance(y_trn, np.ndarray)\", \"y_trn must be a NumPy array.\"),\n",
    "        (\"isinstance(y_vld, np.ndarray)\", \"y_vld must be a NumPy array.\"),\n",
    "        (\"isinstance(y_tst, np.ndarray)\", \"y_tst must be a NumPy array.\"),\n",
    "        (\"X_trn.shape == (1149, 65)\", \"Training data has an incorrect shape, expected (1149, 65).\"),\n",
    "        (\"y_trn.shape == (1149, 10)\", \"Training labels has an incorrect shape, expected (1149, 10).\"),\n",
    "        (\"X_vld.shape == (288, 65)\", \"Validation data has an incorrect shape, expected (288, 65).\"),\n",
    "        (\"y_vld.shape == (288, 10)\", \"Validation labels has an incorrect shape, expected (288, 10).\"),\n",
    "        (\"X_tst.shape == (360, 65)\", \"Testing data has an incorrect shape, expected (360, 65).\"),\n",
    "        (\"y_tst.shape == (360, 10)\", \"Testing labels has an incorrect shape, expected (360, 10).\"),\n",
    "        (\"np.argmax(y_trn, axis=1).sum() == 5166\", \"y_trn had incorrect labels.\"),\n",
    "        (\"np.argmax(y_vld, axis=1).sum() == 1241\", \"y_vld had incorrect labels.\"),\n",
    "        (\"np.argmax(y_tst, axis=1).sum() == 1663\", \"y_tst had incorrect labelss.\"),\n",
    "        # Standardization Tests\n",
    "        ('np.isclose(X_trn[:, 1:].mean(axis=0).sum(), 0, rtol=0.01)', 'X_trn has potentially incorrect mean values.'),\n",
    "        ('np.isclose(X_trn[:, 1:].std(axis=0).sum(), 61, rtol=0.01)', 'X_trn has potentially incorrect std values.'),\n",
    "        ('np.isclose(X_vld[:, 1:].mean(axis=0).sum(), 0.01969, rtol=0.01)', 'X_vld has potentially incorrect mean values.'),\n",
    "        ('np.isclose(X_vld[:, 1:].std(axis=0).sum(), 56.877, rtol=0.01)', 'X_vld has potentially incorrect std values.'),\n",
    "        ('np.isclose(X_tst[:, 1:].mean(axis=0).sum(), -0.64754, rtol=0.01)', 'X_tst has potentially incorrect mean values.'),\n",
    "        ('np.isclose(X_tst[:, 1:].std(axis=0).sum(), 55.0444, rtol=0.01)', 'X_tst has potentially incorrect std values.'),\n",
    "        # Bias Tests\n",
    "        (\"(X_trn[:, 0] == 1).all()\", \"The 1st column of `X_trn` does not seem to be the bias term (i.e., full of 1s).\"),\n",
    "        (\"(X_vld[:, 0] == 1).all()\", \"The 1st column of `X_vld` does not seem to be the bias term (i.e., full of 1s).\"),\n",
    "        (\"(X_tst[:, 0] == 1).all()\", \"The 1st column of `X_tst` does not seem to be the bias term (i.e., full of 1s).\"),\n",
    "    ], **locals())\n",
    "    \n",
    "TEST_get_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8202acf0-709e-4073-a54e-989f13ec20e2",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c5fa867-12a6-4376-8f05-3a06c51fe294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c2bba-0763-4d3a-888f-3620bb9c0ec7",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee33288-c83d-4de0-9b54-1924065ffc85",
   "metadata": {},
   "source": [
    "The function `plot_decision_boundary()` aims to draw the decision boundary using a contour plot. This function will only work when your data has 2 features. Remember, having more than 2-3 features (i.e., higher dimensional features) means we have a much harder time visualizing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e4f8b0-057c-41b6-bb1f-e5df73c6a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(\n",
    "    softreg: object, \n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray,\n",
    "    xlabel: str = '',\n",
    "    ylabel: str = '',\n",
    "    class_names: Dict = None\n",
    ") -> None:\n",
    "    \"\"\" Plots the decision boundry for data with 2 features. \n",
    "    \n",
    "        Warning: \n",
    "            If you have more than 2 features (2D data) the decision boundry\n",
    "            can not be plotted.\n",
    "    \n",
    "        Args:\n",
    "            softreg: An instance of SoftmaxRegression class\n",
    "            \n",
    "            X: Data to be plotted\n",
    "\n",
    "            y: Labels for corresponding data\n",
    "\n",
    "            xlabel: X-axis label for plot\n",
    "\n",
    "            ylabel: Y-axis label for plot\n",
    "            \n",
    "            class_names: Dictionary mapping labels to class names.\n",
    "\n",
    "                Example: {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
    "    \"\"\"\n",
    "    assert X.shape[-1] == 2, f\"`X` must have 2 features not {X.shape[-1]}\"\n",
    "        \n",
    "    if class_names is None:\n",
    "        class_names = {}\n",
    "\n",
    "    # Generate fake data to cover entire space of our input features X\n",
    "    buffer = .5\n",
    "    x_min, x_max = X[:, 0].min() - buffer, X[:, 0].max() + buffer\n",
    "    y_min, y_max = X[:, 1].min() - buffer, X[:, 1].max() + buffer\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, .02),\n",
    "                         np.arange(y_min, y_max, .02))\n",
    "    fake_data = np.c_[xx.ravel(), yy.ravel()].reshape(-1,2)\n",
    "    fake_data =  np.hstack([np.ones((len(fake_data), 1)), fake_data])\n",
    "    # Make prediction\n",
    "    y_hat = softreg.predict(fake_data)\n",
    "\n",
    "    # Plot\n",
    "    plt.contourf(xx, yy, y_hat.reshape(xx.shape))\n",
    "    \n",
    "    labels = np.unique(y)\n",
    "    for l in labels:\n",
    "        class_locs = np.where(y == l)[0]\n",
    "        class_name = class_names.get(l, f'class {l}')\n",
    "        plt.scatter(X[class_locs, 0], X[class_locs, 1], label=class_name)\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb4e86-0537-4ac3-abee-cd194091a3c8",
   "metadata": {},
   "source": [
    "The function `plot_confusion_matrix()`  plots the confusion matrix based on the passed labels and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23b67fcb-c80d-4d59-b6b2-fc52c468b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    y: np.ndarray, \n",
    "    y_hat: np.ndarray, \n",
    "    class_names: Dict[int, str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" Plots a pretty and labeld version of Sklerarn's confusion matrix\n",
    "\n",
    "        Args:\n",
    "            y: Ground truth labels given as a 1D vector\n",
    "\n",
    "            y_hat: Predicted labels given as a 1D vector\n",
    "\n",
    "            class_names: Dictionary mapping labels to class names.\n",
    "\n",
    "                Example: {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
    "\n",
    "        Returns:\n",
    "            A confusion matrix casted as a DataFrame\n",
    "    \"\"\"\n",
    "    y =  y.flatten() # reshape to make 1D vector for consistency\n",
    "    y_hat = y_hat.flatten() # reshape to make 1D vector for consistency\n",
    "    \n",
    "    cfm = confusion_matrix(y_true=y, y_pred=y_hat)    \n",
    "    \n",
    "    labels = np.sort(np.unique(y))\n",
    "    if class_names is not None:\n",
    "        classes = []\n",
    "        for l in labels:\n",
    "            class_name = class_names.get(l, l)\n",
    "            classes.append(class_name)\n",
    "        labels = classes\n",
    "        \n",
    "    columns, index = labels, labels\n",
    "    cfm_df = pd.DataFrame(cfm, index=index, columns=columns)\n",
    "    sns.heatmap(cfm_df, annot=True, fmt='g')\n",
    "    plt.show()\n",
    "    return cfm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1aa8e8-073c-480d-9879-d6dd3b7c1d9e",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e9ad4-a5b8-45b9-833e-9337dadb4e50",
   "metadata": {},
   "source": [
    "#### TODO 4 (10 points): Sigmoid\n",
    "Complete the `sigmoid()` function by converting the below logistic sigmoid equation into code. The function should return an array with the same shape as the input `z`.\n",
    "\n",
    "$$\n",
    "\\large\\begin{align}\n",
    "g(\\zv) &= \\frac{1}{1 + e^{-\\zv}} \\\\\n",
    "&= \\frac{e^{\\zv}}{1 + e^{\\zv}} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Hint**\n",
    "- The $\\frac{e^{z}}{1 + e^{z}}$ version of the sigmoid function can help to prevent any overflow warnings. However, either version of the equation will pass this TODO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65ac85cc-8eb5-40ed-b77c-1f716523cace",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c21ebf12ed4fb9edbca88f6ddad4879",
     "grade": false,
     "grade_id": "cell-cfd1b6c5b50a48f6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    # TODO 4\n",
    "    e_z = np.exp(z)  \n",
    "    return e_z / (1 + e_z)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adffcc13-18ec-4623-925f-a9b0b5ec0949",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "053f0b1ea0cfcf2734060997c28b18fc",
     "grade": true,
     "grade_id": "cell-b83937a464632a40",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_values shape: (120, 1)\n",
      "y_values shape: (120, 1)\n",
      "Your code PASSED all the code checks! \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def TEST_sigmoid():\n",
    "    x_values = np.linspace(-10, 10, 120).reshape(-1, 1)\n",
    "    y_values = sigmoid(x_values)\n",
    "    print(f\"x_values shape: {x_values.shape}\")\n",
    "    print(f\"y_values shape: {y_values.shape}\")\n",
    "\n",
    "    plt.plot(x_values, y_values, '.r')\n",
    "    plt.hlines(y=.5, xmin=x_values.min(), xmax=x_values.max(), \n",
    "               colors='black', linestyles='dotted')\n",
    "    plt.yticks(np.arange(0, 1+.1, .1))\n",
    "    plt.ylabel(\"Probability\")\n",
    "\n",
    "    todo_check([\n",
    "        (\"y_values.shape == (x_values.shape)\", \"sigmoid() output should be the same shape as the input.\"),\n",
    "        (\"np.all(np.isclose(y_values[:3].flatten(), np.array([4.53978687e-05, 5.37059651e-05, 6.35343990e-05]),rtol=.01))\",\n",
    "         \"The returned sigmoid values were incorrect\"),\n",
    "        ('y_values.min() >= 0', \"Minimum value was not 0.\"),\n",
    "        ('y_values.max() <= 1', \"Maximum value was not 1.\"),\n",
    "    ], **locals())\n",
    "TEST_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7209e-ec07-4575-a3ea-2519e9daf4ad",
   "metadata": {},
   "source": [
    "#### TODO 5 (10 points): Softmax\n",
    "Complete the `softmax()` function by converting the below numerically stabilized softmax equation into code. The function should return an array with the same shape as the input `z`.\n",
    "\n",
    "$$\n",
    "\\large\\begin{align}\n",
    "g(\\zv)_i &= \\frac{e^{z_i - z_{max}}}{\\sum_{k=1}^K e^{z_k - z_{max}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Example**\n",
    "\n",
    "Assuming the input `z` is a vector with the shape $(1, |K|)$ or a matrix with the shape $(M , |K|)$ where $M$ is the number of samples, $|K|$ is the number of classes, and $k$/$i$ are index locations. This version of the softmax works by subtracting the max for each row $ z_{max}$ to prevent under or overflow errors.  \n",
    "\n",
    "**Hint**\n",
    "- To compute $z_{max}$ you will need to take the max over all columns, such that each row has a single value. Think about which axis (0 or 1) corresponds to achieving this when computing the max of `z`.\n",
    "- You can use `keepdims=True` argument for many NumPy functions to keep the output dimensions the same as the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a651bb5-d10a-469d-9570-031e2d6fb0e0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "badbc1ee02c01a1a33b662a5a135133f",
     "grade": false,
     "grade_id": "cell-6b511a89267a2376",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Computes the stablized version of the softmax\n",
    "\n",
    "        Args:\n",
    "            z: A vector or matrix of continuous values.\n",
    "\n",
    "        Return:\n",
    "            A NumPy array with the same shape as the input.\n",
    "    \"\"\"\n",
    "    # TODO 5\n",
    "    \n",
    "    e_z = np.exp(z - np.max(z, axis=-1, keepdims=True)) \n",
    "    return e_z / np.sum(e_z, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99462c30-0b76-40a3-b1dc-94854f45aa40",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "993273b22d64786d7b24e4e2f82b1cfa",
     "grade": true,
     "grade_id": "cell-8461902440d05cdb",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape: (1, 3)\n",
      "z:\n",
      "[[0.5488135  0.71518937 0.60276338]]\n",
      "\n",
      "probs shape: (1, 3)\n",
      "probs:\n",
      "[[0.30898065 0.36491138 0.32610797]]\n",
      "\n",
      "Your code PASSED all the code checks! +5 points\n"
     ]
    }
   ],
   "source": [
    "def TEST_softmax_vector():\n",
    "    rng = np.random.RandomState(0)\n",
    "    z = rng.rand(1, 3)\n",
    "    print(f\"z shape: {z.shape}\")\n",
    "    print(f\"z:\\n{z}\\n\")\n",
    "\n",
    "    probs = softmax(z)\n",
    "    print(f\"probs shape: {probs.shape}\")\n",
    "    print(f\"probs:\\n{probs}\\n\")\n",
    "\n",
    "    todo_check([\n",
    "        (\"probs.shape == z.shape\", f\"Expected output shape for softmax() to be same as input {z.shape}\"),\n",
    "        (\"np.isclose(probs.sum(), np.ones(len(z)))\", \"The softmax() rows did not sum to 1.\"),\n",
    "        (\"np.all(np.isclose(probs, np.array([[0.30898065, 0.36491138, 0.32610797]]), rtol=.01))\",\n",
    "         \"softmax_probs has incorrect values\"),\n",
    "    ], **locals(), success_msg=\"+5 points\")\n",
    "    \n",
    "TEST_softmax_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cd3c061-8eb1-4e37-97cc-499e0c254675",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "646c054a9918842314d1c74c9866afa6",
     "grade": true,
     "grade_id": "cell-46e68c2af9d9b44b",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z shape: (5, 3)\n",
      "Z:\n",
      "[[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]\n",
      " [0.38344152 0.79172504 0.52889492]\n",
      " [0.56804456 0.92559664 0.07103606]]\n",
      "\n",
      "probs shape: (5, 3)\n",
      "probs:\n",
      "[[0.30898065 0.36491138 0.32610797]\n",
      " [0.33421115 0.29605481 0.36973403]\n",
      " [0.23434743 0.36907156 0.396581  ]\n",
      " [0.27316453 0.41090326 0.31593222]\n",
      " [0.3291452  0.47062004 0.20023476]]\n",
      "\n",
      "Your code PASSED all the code checks! +5 points\n"
     ]
    }
   ],
   "source": [
    "def TEST_softmax_matrix():\n",
    "    rng = np.random.RandomState(0)\n",
    "    Z = rng.rand(5, 3)\n",
    "    print(f\"Z shape: {Z.shape}\")\n",
    "    print(f\"Z:\\n{Z}\\n\")\n",
    "    \n",
    "    probs = softmax(Z)\n",
    "    print(f\"probs shape: {probs.shape}\")\n",
    "    print(f\"probs:\\n{probs}\\n\")\n",
    "\n",
    "    todo_check([\n",
    "        (\"probs.shape == Z.shape\", f\"Expected output shape for softmax() to be same as input {Z.shape}\"),\n",
    "        (\"np.isclose(probs.sum(axis=1), np.ones(len(Z))).all()\", \"The softmax() rows did not sum to 1.\"),\n",
    "        (\"np.all(np.isclose(probs[:3, 0], np.array([0.30898065, 0.33421115, 0.23434743]), rtol=.01))\",\n",
    "         \"softmax_matrix_probs has incorrecrt values\"),\n",
    "    ], **locals(), success_msg=\"+5 points\")\n",
    "\n",
    "TEST_softmax_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de593e-f76f-42e3-92a4-344ee0fb6a4e",
   "metadata": {},
   "source": [
    "## Negative Log Likelihood (NLL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb729f1b-3e44-4d05-a278-3e5597bcc2f7",
   "metadata": {},
   "source": [
    "#### TODO 6 (10 points): Generalized NLL\n",
    "\n",
    "Complete the `nll_loss()` function by converting the below average NLL loss equation into code where $f(\\xv; \\Wm) = P(y \\mid \\xv; \\Wm)$ are the predicted probabilities. This function should return a float.\n",
    "\n",
    "$$\n",
    "NLL(\\Wm) = - \\frac{1}{M}\\sum_{m=1}^{M} {\\yv_m} * \\log[f(\\xv_m;\\Wm)]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "520d27b2-65c3-473a-ba68-4694c67898f5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fc1f245ec2be658571fe37e1be2714a",
     "grade": false,
     "grade_id": "cell-b40fd4a140731442",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def nll_loss(y: np.ndarray, probs: np.ndarray) -> float:\n",
    "    \"\"\" Computes the average generalized NLL Loss\n",
    "\n",
    "        Args:\n",
    "            y: The ground truth one-hot encoded labels\n",
    "                given as matrix of shape (M, |K|).\n",
    "\n",
    "            probs: The predicted probabilities for the\n",
    "                corresponding labels given as a matrix\n",
    "                of shape (M, |K|)\n",
    "    \"\"\"\n",
    "    # TODO 6\n",
    "    return (-1/len(y))*np.sum(y*np.log(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28b22584-fc43-4b33-bac6-cd296e5352c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a76a90205e81851a40ecc86bc2e4a44",
     "grade": true,
     "grade_id": "cell-a1140c48bfde35fa",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averge NLL Loss: 0.7512649762748712\n",
      "Your code PASSED all the code checks! \n"
     ]
    }
   ],
   "source": [
    "def TEST_nll_loss():\n",
    "    y = np.array([\n",
    "        [0., 1., 0.],\n",
    "        [0., 0., 1.],\n",
    "        [1., 0., 0.]\n",
    "    ])\n",
    "    y_hat_probs = np.array([\n",
    "        [.1, .5, .4],\n",
    "        [.1, .2, .7],\n",
    "        [.3, .45, .25],\n",
    "    ])\n",
    "    avg_loss = nll_loss(y=y, probs=y_hat_probs)\n",
    "    print(f\"Averge NLL Loss: {avg_loss}\")\n",
    "\n",
    "    todo_check([\n",
    "        (\"np.isclose(avg_loss, 0.7512649, rtol=.01)\", \"Incorrect value for `nll_loss()` output\")\n",
    "    ], **locals())\n",
    "\n",
    "TEST_nll_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d953a6-63e0-4fb4-8b7e-055d0d417010",
   "metadata": {},
   "source": [
    "## Coding Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dddc66",
   "metadata": {},
   "source": [
    "#### TODO 7 (20 points): Softmax Regression \n",
    "Compete the TODO by finishing the `SoftmaxRegression` class implementation.\n",
    "\n",
    "**`fit()` TODOs**\n",
    "\n",
    "Follow the below steps to implement the `fit()` method. Feel free to reuse code from prior homework.\n",
    "\n",
    "1. Implement the `fit()` function according to the pseudocode provided below.\n",
    "    1. Hint: Subscript $b$ corresponding to index samples at current batch, while $B$ corresponds to the size of the batch (i.e., number of samples in the batch).\n",
    "    2. Lines 11-15 will be used to measure and store the training/validation NLL loss for the current epoch. Learning curves will not plot without this!\n",
    "<center><img src=\"https://live.staticflickr.com/65535/54132218806_ddfa050d7c_c.jpg\"/></center>\n",
    "\n",
    "Follow the below steps to implement the `predict()` method. Feel free to reuse code from prior homework.\n",
    "\n",
    "2. Compute the predictions by implementing the below equation. Return the predictions as a 2D column vector.\n",
    "  \n",
    "$$\n",
    " \\hat{\\yv}  = \\arg \\max(\\Xm \\Wm)\n",
    "$$\n",
    "\n",
    "**Hint**\n",
    "- You do not need to reimplement any equations in the code that we have already implemented (e.g., NLL loss). simply reuse those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fc5400d-e6e5-447c-adff-872160b2c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(\n",
    "    data_len: int, \n",
    "    batch_size: int = 32,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\" Generates mini-batches based on the data indexes\n",
    "        \n",
    "        Args:\n",
    "            data_len: Length of the data or number of data samples \n",
    "                in the data. This is used to generate the indices of\n",
    "                the data.\n",
    "            \n",
    "            batch_size: Size of each mini-batch where the last mini-batch\n",
    "                might be smaller than the rest if the batch_size does not \n",
    "                evenly divide the data length.\n",
    "\n",
    "        Returns:\n",
    "            A list of NumPy array's holding the indices of batches\n",
    "    \"\"\"\n",
    "    indices = np.arange(data_len)\n",
    "    np.random.shuffle(indices)\n",
    "    batches = [indices[i:i+batch_size] for i in range(0, data_len, batch_size)]\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "484363fb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "344061ce16f69368aa719adbb9c11b96",
     "grade": false,
     "grade_id": "cell-9d1773d60d8b0785",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SoftmaxRegression():\n",
    "    \"\"\" Performs softmax regression using gradient descent\n",
    "    \n",
    "        Attributes:\n",
    "\n",
    "            alpha: learning rate or step size.\n",
    "                \n",
    "            batch_size: Size of mini-batches for mini-batch gradient\n",
    "                descent.\n",
    "            \n",
    "            epochs: Number of epochs to run for mini-batch\n",
    "                gradient descent.\n",
    "                \n",
    "            seed: Seed to be used for NumPy's RandomState class\n",
    "                or universal seed np.random.seed() function.\n",
    "\n",
    "            W: Matrix of weights with shape (N, |K|) \n",
    "\n",
    "            trn_loss: Stores the training loss for each epoch.\n",
    "\n",
    "            vld_loss: Stores the validation loss for each epoch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        alpha: float,\n",
    "        batch_size: int,\n",
    "        epochs: int = 1,\n",
    "        seed: int = 0,\n",
    "    ):\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.W = None\n",
    "        self.trn_loss = None\n",
    "        self.vld_loss = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, X_vld: np.ndarray = None, y_vld: np.ndarray = None) -> object:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: Training features/data\n",
    "            y: Training labels\n",
    "            X_vld: Validation features/data\n",
    "            y_vld: Validation labels\n",
    "            seed: Seed used for initializing weights\n",
    "        \"\"\"\n",
    "        # Set seed for reproducibility\n",
    "        np.random.seed(self.seed)\n",
    "        # Track training/validation loss\n",
    "        self.trn_loss = []\n",
    "        self.vld_loss = []\n",
    "    \n",
    "        # Number of classes\n",
    "        n_classes = np.max(y) + 1\n",
    "    \n",
    "        # Initialize weights with shape (n_features, n_classes)\n",
    "        self.W = np.random.randn(X.shape[1], n_classes) * 0.01\n",
    "    \n",
    "        for epoch in range(self.epochs):\n",
    "            batches = get_batches(X.shape[0], self.batch_size)\n",
    "            for batch in batches:\n",
    "                X_batch, y_batch = X[batch], y[batch]\n",
    "                \n",
    "                # Ensure y_batch is of integer type for indexing\n",
    "                y_batch = y_batch.astype(int)\n",
    "    \n",
    "                logits = X_batch.dot(self.W)  # (batch_size, n_classes)\n",
    "                probs = softmax(logits)      # (batch_size, n_classes)\n",
    "    \n",
    "                # One-hot encode labels\n",
    "                y_one_hot = np.zeros((len(y_batch), n_classes))  # Shape (batch_size, n_classes)\n",
    "                for i in range(len(y_batch)):\n",
    "                    y_one_hot[i, y_batch[i]] = 1\n",
    "    \n",
    "                # Compute gradient and update weights\n",
    "                grad = X_batch.T.dot(probs - y_one_hot) / len(y_batch)\n",
    "                self.W -= self.alpha * grad\n",
    "    \n",
    "            # Track training loss\n",
    "            logits_trn = X.dot(self.W)      # Full dataset logits\n",
    "            probs_trn = softmax(logits_trn)  # Full dataset probabilities\n",
    "            trn_loss = nll_loss(y, probs_trn, n_classes)\n",
    "            self.trn_loss.append(trn_loss)\n",
    "    \n",
    "            # Track validation loss if provided\n",
    "            if X_vld is not None and y_vld is not None:\n",
    "                logits_vld = X_vld.dot(self.W)      # Validation logits\n",
    "                probs_vld = softmax(logits_vld)    # Validation probabilities\n",
    "                vld_loss = nll_loss(y_vld, probs_vld, n_classes)\n",
    "                self.vld_loss.append(vld_loss)\n",
    "    \n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Make predictions using learned weights\n",
    "\n",
    "            Args:\n",
    "                X: Testing data given as a 2D matrix\n",
    "\n",
    "            Returns:\n",
    "                A 2D column vector of predictions for each data sample in X\n",
    "        \"\"\"\n",
    "        # TODO 7.2\n",
    "        logits = X.dot(self.W)\n",
    "        probabilities = softmax(logits)\n",
    "        return np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5259e21e-6a88-4154-b575-4a47ceb3f864",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45c4f2be6868140af24a9b8009b0bb7e",
     "grade": true,
     "grade_id": "cell-c54f4aedfd1844a8",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 39\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msoftreg\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m     todo_check([\n\u001b[1;32m     33\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen(softreg.trn_loss) == 100\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 100 training evaluations for `self.trn_loss`. Make sure you added code for tracking the training accuracy and added the convergence check.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     34\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen(softreg.vld_loss) == 100\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 100 validation evaluations for `self.vld_loss`. Make sure you added code for tracking the validation accuracy and added the convergence check.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     35\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc == .98\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect accuracy value\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     36\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.all(np.isclose(softreg.W.flatten()[:3], np.array([ -0.29371,  0.95801,  1.39295]), rtol=.01))\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect weight values.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m     ], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlocals\u001b[39m())\n\u001b[0;32m---> 39\u001b[0m \u001b[43mTEST_SoftmaxRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m, in \u001b[0;36mTEST_SoftmaxRegression\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m X_trn, X_vld, y_trn, y_vld \u001b[38;5;241m=\u001b[39m train_test_split(X, y, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     14\u001b[0m softreg \u001b[38;5;241m=\u001b[39m SoftmaxRegression(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.1\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43msoftreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_vld\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_vld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_vld\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_vld\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m softreg\u001b[38;5;241m.\u001b[39mpredict(X_vld)\n\u001b[1;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(softreg\u001b[38;5;241m.\u001b[39mtrn_loss, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 59\u001b[0m, in \u001b[0;36mSoftmaxRegression.fit\u001b[0;34m(self, X, y, X_vld, y_vld)\u001b[0m\n\u001b[1;32m     56\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(y) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Initialize weights with shape (n_features, n_classes)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m     62\u001b[0m     batches \u001b[38;5;241m=\u001b[39m get_batches(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:1310\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randn\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:1471\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.standard_normal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:657\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "def TEST_SoftmaxRegression():\n",
    "    from sklearn.datasets import make_blobs\n",
    "    X, y = make_blobs(\n",
    "        n_samples=1000,\n",
    "          random_state=42,  \n",
    "          centers=[[2, 1.5], [-1.5, -1], [1, -1]],\n",
    "          cluster_std=0.6\n",
    "    )\n",
    "    n_values = np.max(y) + 1\n",
    "    y = np.eye(n_values)[y.flatten()]\n",
    "    X = np.hstack([np.ones([len(X), 1]), X])\n",
    "    X_trn, X_vld, y_trn, y_vld = train_test_split(X, y, random_state=42)\n",
    "\n",
    "    softreg = SoftmaxRegression(epochs=100, batch_size=64, alpha=.1, seed=42)\n",
    "    softreg.fit(X_trn, y_trn, X_vld=X_vld, y_vld=y_vld)\n",
    "    y_hat = softreg.predict(X_vld)\n",
    "    \n",
    "    plt.plot(softreg.trn_loss, label='Train loss')\n",
    "    plt.plot(softreg.vld_loss, label='Validation loss')\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.ylabel(\"Average NLL Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plot_decision_boundary(softreg, X[:, 1:], np.argmax(y, axis=1))\n",
    "\n",
    "    acc = accuracy_score(np.argmax(y_vld, axis=1), y_hat)\n",
    "    print(f\"Validation Accuracy: {acc}\")\n",
    "    print(f\"Weights: {softreg.W.flatten()}\")\n",
    "\n",
    "    todo_check([\n",
    "        (\"len(softreg.trn_loss) == 100\", \"Expected 100 training evaluations for `self.trn_loss`. Make sure you added code for tracking the training accuracy and added the convergence check.\"),\n",
    "        (\"len(softreg.vld_loss) == 100\", \"Expected 100 validation evaluations for `self.vld_loss`. Make sure you added code for tracking the validation accuracy and added the convergence check.\"),\n",
    "        (\"acc == .98\", \"Incorrect accuracy value\"),\n",
    "        (\"np.all(np.isclose(softreg.W.flatten()[:3], np.array([ -0.29371,  0.95801,  1.39295]), rtol=.01))\", \"Incorrect weight values.\")\n",
    "    ], **locals())\n",
    "       \n",
    "TEST_SoftmaxRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a131c-2c46-48b9-82f3-581050073c54",
   "metadata": {},
   "source": [
    "## MNIST Data With Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab8df3-760f-40de-8c59-51a10cadb7ca",
   "metadata": {},
   "source": [
    "#### TODO 8 (25 points)\n",
    "To complete this TODO by performing multi-classification on the MNIST dataset.\n",
    "\n",
    "- Data Loading\n",
    "    - Load the training, validation, and test datasets.\n",
    "- Training\n",
    "    - Initialize an instance of `SoftmaxRegression()` and fit it on the training data.\n",
    "    - Compute and display the training accuracy.\n",
    "    - Plot the learning curve, which should include the training and validation losses for each epoch.\n",
    "    - Plot the confusion matrix for the training data.\n",
    "- Validation\n",
    "    - Compute and display the validation accuracy.\n",
    "    - Plot the confusion matrix for the validation data.\n",
    "- Testing\n",
    "    - Compute and display the testing accuracy.\n",
    "    - Plot the confusion matrix for the testing data.\n",
    "  \n",
    "**Hints**\n",
    "\n",
    "- To compute the accuracy and confusion matrix, you will need to undo the one-hot encodings. You can do so by taking the $\\arg \\max$ over the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10b41b0-4c0e-45e1-a263-c8e5d721d915",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724e3ea-14cc-44f3-b7c7-fbf0e9b64e3d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39fd54b96676ed07de7c6c134cce201e",
     "grade": true,
     "grade_id": "cell-0786d80261d144d9",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### TODO 8: Training Code\n",
    "\n",
    "X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = get_preprocessed_data()\n",
    "\n",
    "softreg = SoftmaxRegression(alpha=0.1, batch_size=64, epochs=100, seed=42)\n",
    "softreg.fit(X_trn, y_trn, X_vld=X_vld, y_vld=y_vld)\n",
    "\n",
    "y_trn_pred = softreg.predict(X_trn)\n",
    "train_accuracy = accuracy_score(np.argmax(y_trn, axis=1), y_trn_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565cde8-acd2-427f-913f-b5bf58699c12",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8c06d-49e9-49e0-b549-5c9d88149f68",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da4dd6a88b08529bd64a016bbffbe6ef",
     "grade": true,
     "grade_id": "cell-e3d25a878292c097",
     "locked": false,
     "points": 7.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 8: Validation Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4504b30-75f5-40c7-9e84-a873e5a0917a",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08196a3-8b57-4ed6-8a82-659de9930b43",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "009bdda90971dc45141e32536413b899",
     "grade": true,
     "grade_id": "cell-49af7ee4f2538d15",
     "locked": false,
     "points": 7.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 8: Testing Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1c98d-847d-4834-beee-275dbb50a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "I was unable to implement SoftmaxRegression and attempted to utilize chatgbt to aid in the implementation which also did not work\n",
    "The final implementations were equally unsuccessful. \n",
    "\n",
    "Understand that this is a poor performance for me. I am sorry. \n",
    "Thank you again for the extension period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a672d1-f7b3-4e9c-8078-0fe2793109b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
